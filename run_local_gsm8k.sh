#!/usr/bin/env bash
set -euo pipefail

# æœ¬åœ° llama.cpp server çš„è®¾ç½®
LOCAL_API_BASE="${LOCAL_API_BASE:-http://127.0.0.1:8000/v1}"
LOCAL_API_KEY="${LOCAL_API_KEY:-sk-local}"
LOCAL_MODEL_ID="${LOCAL_MODEL_ID:-qwen2.5-7b-instruct}"

# è¯„æµ‹é…ç½®
TASKS="${1:-gsm8k}"             # åªè·‘ gsm8kï¼Œé¿å… loglikelihood æŠ¥é”™
BATCH_SIZE=1                    # chat-completions ä¸æ”¯æŒå¹¶å‘ batchï¼Œå›ºå®š 1
SEED="${SEED:-42}"
TEMP="${TEMP:-0}"
TOP_P="${TOP_P:-1}"
MAX_NEW_TOKENS="${MAX_NEW_TOKENS:-64}"
LIMIT="${LIMIT:-}"              # æ¯”ä¾‹(0.02/0.1) æˆ– æ•°å­—(27/132)ã€‚ç•™ç©º=å…¨é‡

# è¾“å‡ºç›®å½•
OUTDIR="$(dirname "$0")/../outputs"
mkdir -p "$OUTDIR"
STAMP="$(date +%Y%m%d_%H%M%S)"
OUTFILE="$OUTDIR/local_${LOCAL_MODEL_ID//[^a-zA-Z0-9_]/_}_${TASKS//[, ]/_}_${STAMP}.json"

# å…ˆæ£€æŸ¥æœ¬åœ°æœåŠ¡æ˜¯å¦åœ¨çº¿
if ! curl -sS "${LOCAL_API_BASE}/models" -H "Authorization: Bearer ${LOCAL_API_KEY}" >/dev/null ; then
  echo "âŒ æ— æ³•è¿åˆ°æœ¬åœ° llama.cpp server: ${LOCAL_API_BASE}"
  echo "ğŸ‘‰ è¯·å…ˆå¯åŠ¨ï¼š"
  echo "   python -m llama_cpp.server \\"
  echo "     --model /path/to/your.gguf \\"
  echo "     --model_alias ${LOCAL_MODEL_ID} \\"
  echo "     --host 127.0.0.1 --port 8000 \\"
  echo "     --n_ctx 4096 --n_threads \$(nproc) --n_gpu_layers 0"
  exit 1
fi

# è¿™äº›ä»»åŠ¡ä¼šè§¦å‘ loglikelihoodï¼ˆchat-completions ä¸æ”¯æŒï¼‰ï¼Œåœ¨æœ¬åœ°/DeepSeekéƒ½ä¼šæŠ¥é”™
BAD_TASKS_REGEX='(^|,)(ceval|mmlu|cmmlu|arc_|hellaswag|piqa|boolq|winogrande|openbookqa|lambada|wikitext|ptb|c4)(,|$)'
if [[ "$TASKS" =~ $BAD_TASKS_REGEX ]]; then
  echo "âŒ æ£€æµ‹åˆ°éœ€è¦ loglikelihood çš„ä»»åŠ¡ï¼š$TASKS"
  echo "ğŸ‘‰ è¯·é€‰æ‹©ç”Ÿæˆå¼ä»»åŠ¡ï¼ˆå¦‚ gsm8kï¼‰ï¼Œæˆ–æ”¹ç”¨æ”¯æŒ logprobs çš„åç«¯ã€‚"
  exit 1
fi

CMD=(python -m lm_eval
  --model openai-chat-completions
  # æ˜ç¡®æŒ‡å®šåˆ°æœ¬åœ° server çš„ chat-completions å®Œæ•´è·¯å¾„ï¼Œå¹¶æä¾›æœ¬åœ°â€œå¯†é’¥â€
  --model_args "model=${LOCAL_MODEL_ID},base_url=${LOCAL_API_BASE}/chat/completions,api_base=${LOCAL_API_BASE}/chat/completions,api_key=${LOCAL_API_KEY}"
  --tasks "$TASKS"
  --batch_size "$BATCH_SIZE"
  --seed "$SEED"
  --apply_chat_template
  --gen_kwargs "temperature=${TEMP},top_p=${TOP_P},max_new_tokens=${MAX_NEW_TOKENS}"
  --system_instruction "Solve step by step, and print ONLY the final numeric answer on the last line as: #### <number>."
  --output_path "$OUTFILE"
)

# åŒé‡å¯¹é½ï¼šè®¾ç½® LIMITï¼ˆæ¯”ä¾‹æˆ–æ•´æ•°ï¼‰ï¼Œæ¯”å¦‚ LIMIT=0.02 / LIMIT=27
if [[ -n "${LIMIT}" ]]; then
  CMD+=(--limit "$LIMIT")
fi

# è§£é™¤å¯èƒ½æŠŠè¯·æ±‚å‘åˆ° openai.com çš„ç¯å¢ƒå˜é‡å¹²æ‰°
env -u OPENAI_BASE_URL -u OPENAI_API_BASE -u OPENAI_API_KEY -u OPENAI_ORG_ID -u OPENAI_API_TYPE \
  "${CMD[@]}"

echo "âœ… å·²å®Œæˆï¼š$OUTFILE"
